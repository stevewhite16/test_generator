id,type,title,text,field,subfield
1,passage,The Ultimatum Game,"Imagine that someone offers you and some other anonymous person $100 to share. The rules are strict and known to both players. The two of you are in separate rooms and cannot exchange information. A coin toss decides which of you will propose how to share the money. If you are the proposer you can make a single offer of how to split the sum, and the other person—the responder—can say yes or no. If the responder’s answer is yes, the deal goes ahead. If the answer is no, neither of you gets anything. In both cases, the game is over and will not be repeated. What will you do?
Instinctively, many people feel they should offer 50 percent, because such a division is “fair” and therefore likely to be accepted. More daring people, however, think they might get away with offering somewhat less than half of the sum.
You may not be surprised to learn that two-thirds of the offers are between 40 and 50 percent. Only four in 100 people offer less than 20 percent. Proposing such a small amount is risky, because it might be rejected. More than half of all responders reject offers that are less than 20 percent. But why should anyone reject an offer as “too small”? The responder has just two choices: take what is offered or receive nothing. The only rational option for a selfish individual is to accept any offer. A selfish proposer who is sure that the responder is also selfish will therefore make the smallest possible offer and keep the rest. This game-theory analysis, which assumes that people are selfish and rational, tells you that the proposer should offer the smallest possible share and the responder should accept it. But this is not how most people play the game.
The scenario just described, called the Ultimatum Game, was devised some twenty years ago. Experimenters subsequently studied the Ultimatum Game intensively in many places using diverse sums. The results proved remarkably robust. Behavior in the game did not appreciably depend on the players’ sex, age, schooling, or numeracy. Moreover, the amount of money involved had surprisingly little effect on results. Yet the range of players remained limited, because the studies primarily involved people in more developed countries and often university students.
Recently, a cross-cultural study in fifteen small-scale societies showed that there were sizable differences in the way some people play the Ultimatum Game. Within the Machiguenga tribe (from the Amazon) the mean offer was considerably lower than in typical Western-type civilizations—26 percent instead of 45 percent. Conversely, many members of the Au tribe (from Papua New Guinea) offered more than 50 percent. Cultural traditions in gift giving, and the strong obligations that result from accepting a gift, play a major role among some tribes, such as the Au. Yet despite these cultural variations, the outcome was always far from what rational analysis would dictate for selfish players. Most people all over the world place a high value on fair outcomes.
For a long time, theoretical economists postulated a being called Homo economicus—a rational individual relentlessly bent on maximizing a purely selfish reward. But the lesson from the Ultimatum Game and similar experiments is that real people are a crossbreed of H. economicus and H. emoticus, a complicated hybrid species that can be ruled as much by emotion as by cold logic and selfishness. An interesting challenge is to understand how Darwinian evolution would produce creatures instilled with emotions and behaviors that do not immediately seem geared toward reaping the greatest benefit for individuals or their genes.
Adapted from K. Sigmund, E. Fehr, and M.A. Nowak, “The economics of fair play.” ©2002 by Scientific American, Inc.

",social science,economics
2,passage,Living in a Rational Society,"The rationalizing of society can be conceptualized as the pursuit of efficiency, predictability, calculability, and control through technology. But rational systems inevitably spawn a series of irrationalities that result in the compromising and perhaps even the undermining of their rationality.
Fast-food restaurants, which epitomize the rational model, proffer the fastest means of getting from a hungry state to a sated one, without surprises, at low cost, in a carnival-like setting suggesting that fun awaits the consumer at each visit. The wholesomeness of the food seems an insignificant consideration. Whereas in the past, working people were prepared to spend up to an hour preparing dinner, they now are impatient if a meal is not on the table within ten minutes. (For their part, some fast-food restaurants have developed chairs that become uncomfortable after about twenty minutes, to ensure that diners do not stay long.)
Fast-food restaurants have preferentially recruited adolescent help, at least until recently, because this age group adjusts more easily than adults do to surrendering their autonomy to machines, rules, and procedures. Few skills are required on the job, so workers are asked to use only a minute portion of their abilities. This policy is irrational from the standpoint of the organization, since it could obtain much more from its employees for the money (however negligible) it pays them. These minimal skill demands are also irrational from the perspective of the employees, who are not allowed to think or to respond creatively to the demands of the work.
These restrictions lead to high levels of resentment, job dissatisfaction, alienation, absenteeism, and turnover among workers in fast-food franchises. In fact, these businesses have the highest turnover rate of any industry in the U.S. The entire workforce of the fast-food industry turns over three times in a year. Although the simple, repetitive nature of the work makes it easy to replace those who leave, the organization would clearly benefit from keeping employees longer. The costs of hiring and training are magnified when the turnover rate is extraordinarily high.
The application of the rational model to the house-building process in the 1950s and ’60s led to suburban communities consisting of nearly identical structures. Indeed, it was possible to wander into the residence of someone else and not to realize immediately that one was not at home. The more expensive developments were superficially more diversified, but their interior layouts assumed residents who were indistinguishable in their requirements.
Furthermore, the planned communities themselves look very similar. Established trees are bulldozed to facilitate construction. In their place, a number of saplings, held up by posts and wire, are planted. Streets are laid out in symmetrical grid patterns. With such uniformity, suburbanites may well enter the wrong subdivision or become lost in their own.
Many of Steven Spielberg’s films are set in such suburbs. Spielberg’s strategy is to lure the viewer into this highly repetitive world and then to have a completely unexpected event occur. For example, the film Poltergeist takes place in a conventional suburban household in which evil spirits ultimately disrupt the sameness. (The spirits first manifest themselves through another key element of the homogeneous society—the television set.) The great success of Spielberg’s films may be traceable to a longing for some unpredictability, even if it is bizarre and menacing, in increasingly routinized lives.
Adapted from G. Ritzer, The McDonaldization of Society. ©1993 by Pine Forge Press.

",social science,sociology
3,passage,"Deconstruction and Literature
","Deconstructionism, as applied to literary criticism, is a paradox about a paradox: It assumes that all discourse, even all historical narrative, is essentially disguised self-revelatory messages. Being subjective, the text has no fixed meaning, so when we read, we are prone to misread. Deconstructionism emerged from Paris and, notwithstanding its claim to universality, has an evident history. It is a manifestation of existential anxieties about presence and absence, reality and appearance. It developed via structuralism, with its emphasis on semantics and symbolism.
From these sources it derived its fundamental premise: the endless slippage of the subject, the futility of any attempt to name reality. The premise suggests the disillusion attendant on the collapse of the two major forces in twentieth-century European thought: enlightened humanism and idealistic Marxism. Despite its origins, deconstructionism found its own best home in the United States, that historically dissociated construction of random meanings. (“America is deconstruction,” said its leading proponent, Jacques Derrida.)
By the 1970s, deconstruction filled—perhaps better, emptied—the gap left in the humanities in the U.S. by the demise of the old “new criticism.” But what began as brilliant and creative analytic performances soon became classroom pedagogy. Throughout the decade, the seminar rooms on U.S. campuses—and then campuses worldwide—became workshops in deconstructionist practice. Junior misreaders worked away, becoming ever more like C.I.A. operatives, decoding false signals sent by a distant enemy, the writer.
Deconstruction exalted itself with ever higher pretensions. As one academic critic exulted, “The history of literature is part of the history of criticism.” Deconstruction transformed everything into social commentary, easily making affinities with sexual and racial politics, two other militant philosophies that challenge the sanctity of text. It presented itself as a supra-ideological mode of analysis, exposing the ideological aberrations of others while seemingly possessing none itself.
Any resistance that deconstructionists encountered was usually interpreted as censorious ignorance. As their approach prevailed, gangs of neodeconstructionists descended on the library with their critical services. One would demythologize, another decanonize, another dephallicize, another dehegemonize, another defame. Literature, as the deconstructionists frequently proved, had been written by entirely the wrong people for entirely the wrong reasons. Soon all that would be left of it would be a few bare bones of undecidable discourse and some tattered leather bindings. This frenzy would be called a conference of the Modern Language Association.
The point that needs to be reaffirmed is that writing is an existential act, an imaginative exploration of ideas. It is, in fact, an expression of moral responsibility. Literature is not a subordinate category of social criticism. When writers are censored, imprisoned, killed, or threatened with death for their writings, it is not because their discourse is undecidable. If we are to take authors and their fate seriously, we must recognize that fiction is more than an opportunity for word games; we must honor it as a mode of radical discovery.
We need an ambiance around writing that affirms its nature as creativity, as art, and that in a larger sense considers creativity a prime power in the making of intelligence, feeling, and morality. This was the position from which Jean-Paul Sartre with his freedom-affirming existentialism started the postwar debate of which deconstruction is a latter-day development. He started it because during the 1930s the word had been defamed and disfigured, the book burned, the writer erased, by forces that lay outside criticism, in history.
Adapted from M. Bradbury, The scholar who misread history. ©1991 by New York Times.

",humanities,literature
4,passage,"Tools for Thought
","The tools we use to think change the ways in which we think. The invention of written language brought about a radical shift in how we process, organize, store, and transmit representations of the world. Although writing remains our primary information technology, today when we think about the impact of technology on our habits of mind, we think primarily of the computer.
My first encounters with how computers change the way we think came soon after I joined the faculty at the Massachusetts Institute of Technology at the end of the era of the slide rule and the beginning of the era of the personal computer. At a lunch for new faculty members, several senior professors in engineering complained that the transition from slide rules to calculators had affected their students’ ability to deal with issues of scale. When students used slide rules, they had to insert decimal points themselves. The professors insisted that doing that required students to maintain a mental sense of scale, whereas those who relied on calculators made frequent errors in orders of magnitude. Additionally, the students with calculators had lost their ability to do “back of the envelope” calculations, and with that, an intuitive feel for the material.
That same semester, I taught a course in the history of psychology. There, I experienced the impact of computational objects on students’ ideas about their emotional lives. My class had read Freud’s essay on slips of the tongue, with its famous first example: The chair of a parliamentary session opens a meeting by declaring it closed. The students discussed how Freud interpreted such errors as revealing a person’s mixed emotions. A computer science major disagreed with Freud’s approach. The mind, she argued, is a computer. And in a computational dictionary—like we have in the human mind—closed and open are designated by the same symbol, separated by a sign for opposition. Closed equals minus open. To substitute closed for open does not require the notion of ambivalence or conflict. “When the chairman made that substitution,” she declared, “a bit was dropped; a minus sign was lost. There was a power surge. No problem.” The young woman turned a Freudian slip into an information-processing error. An explanation in terms of meaning had become an explanation in terms of mechanism.
Today, starting in elementary school, students use e-mail, word processing, computer simulations, and virtual communities. In the process, they are absorbing more than the content of what appears on their screens. They are learning new ways to think about what it means to know and understand.
There are a number of areas where I see information technology encouraging changes in thinking. There can be no simple way of cataloging whether any particular change is good or bad. That is contested terrain. At every step we have to ask, as educators and citizens, whether current technology is leading us in directions that serve our human purposes. Such questions are not technical; they are social, moral, and political. For me, addressing that subjective side of computation is one of the more significant challenges for the next decade of information technology in higher education. Technology does not determine change, but it encourages us to take certain directions. If we make those directions clear, we can more easily exert human choice.
Adapted from S. Turkle, How computers change the way we think. ©2004 by The Chronicle of Higher Education.
",?,?
5,passage,Does Free Will Exist?,"Is there such a thing as free will? Perhaps the philosopher who has gotten closest to a sensible understanding of free will is Daniel C. Dennett, who thinks of the phenomenon as “the power to veto our urges and then to veto our vetoes . . . the power of imagination, to see and imagine futures.”
Over the last few decades, science has made small but significant advances in understanding the relationship between conscious and unconscious thought, and the data are beginning to paint a picture that seems to validate Dennett’s views. In the 1970s, Benjamin Libet wired people to an electroencephalogram and measured when they reported having a particular conscious thought about an action and when the nerve impulses corresponding to the initiation of the actual action started. Astoundingly, the subjects had actually made (unconsciously) the decision to act measurably earlier than when they became aware of it consciously. The conscious awareness, in a sense, was a “story” that the higher cognitive parts of the brain told to account for the action.
So science may be showing us that free will is more a feeling than a real manifestation of independent will. As psychologist Dan Wegner put it, “We see two tips of the iceberg, the thought and the action, and we draw a connection.” Libet’s position is a bit more moderate and is akin to Dennett’s. Libet says that free will is a form of veto power, filtering and sometimes blocking decisions provisionally made at the unconscious level.
Quantum mechanics is sometimes brought into discussions of free will by supporters of pseudoscience because it is very technical and, more important, incomprehensible enough to lend that aura of scientific credibility without committing one to specific details. Some philosophers and scientists suggest that perhaps free will can be explained by occasional quantum fluctuations that, by interfering with subcellular phenomena in the brain, create a partial decoupling of our decision-making processes from the standard macroscopic laws of causality. This is nonsense, not only because we have absolutely no evidence of “quantum fluctuations” (whatever they are) at the brain level, but because, even if they did happen, they would—at most—generate random, not free, will. And random will is not one of those varieties of free will that is, in Dennett’s words, “worth having.”
Another source of confusion between science and philosophy when it comes to free will is to be found in the rather vague concept of “emergent properties,” for example, the notion of free will being an emergent property of the higher brain’s functions. Even though some scientists are predisposed to reject emergent properties, “emergence” can actually be studied scientifically and is a rather common phenomenon. For example, when hydrogen and oxygen combine to form water, the resulting molecule has emergent physical–chemical properties, in the sense that the temperatures marking transition states are not simple functions of the properties of the individual atoms.
Some philosophers have argued that emergence restricts the limits of reductionism, not because it isn’t “physics all the way down,” but because, frankly, a quantum mechanical description of, say, the Brooklyn Bridge isn’t going to be very helpful. Emergence entails that certain phenomena are best studied, and understood, at some levels of analysis rather than others, and free will may well fall into this category. To say that it is an emergent property of the brain is not a call for magic or pseudoscience, just the realization that neurobiology and psychology are better positioned than quantum mechanics to understand it.
Adapted from M. Pigliucci, Can there be a science of free will? ©2007 the Committee for Skeptical Inquiry.

",humanities,philosophy
6,passage,The Honest Truth about Dishonesty,"Although business people deserve more respect for their honesty than they receive, a common complaint is that they take advantage of consumers through dishonest advertising. Instead of providing useful information for making rational choices, advertisements often appeal to consumers’ emotions to persuade them to buy products regardless of need. This complaint is true and obvious to all but the most naïve people. Advertisements are designed to convince consumers to favor one product over others, and presenting solely unbiased and unemotional information would seldom be the best way to accomplish this goal.
Thoughtful people recognize that politicians advertise themselves and their policy recommendations in similarly biased and emotional ways. The question is not whether businesspeople or politicians have the strongest moral commitment to truthfulness in advertising. Both groups will deviate from honest practices when they expect that the benefits of doing so will exceed the costs. The important question is “Who can most easily mislead their customers with emotional statements, unrealistic promises, and biased information: businesspeople or politicians?”
People are less likely to be swayed by dishonesty and emotion when responding to business ads than when responding to political ads for two reasons. First, businesspeople are attempting to persuade people who are usually spending their own money; politicians are trying to persuade people who are deciding how they want to spend other people’s money. The motivation to minimize mistakes by carefully considering claims about costs and benefits before a decision is made and by evaluating those claims in light of post-decision experience is greater when one is bearing all of the cost of the decision than when others are bearing most of the cost.
The second reason why misleading claims are less effective in promoting commercial products than in promoting political products is because the choices that consumers of commercial products make have more decisive effects on outcomes than do the choices of consumers of political products. When people purchase a product in the marketplace, they get the product they choose, and they get it because they chose it. The probability that a voter’s choice will be decisive is increasingly small in state and federal elections, and seldom greater than a fraction of one percent in most local elections. Given such a low probability of any one person’s vote determining the outcome of the election, voters have little motivation to be concerned about the accuracy of the political claims being made.
One might think that professors would be more honest than both businesspeople and politicians when promoting their products’ value (that is, in their teaching and research). Unlike politicians, professors try to sell their products to customers who can decisively accept or reject them without being directly affected by how many others make different choices. However, many undergraduate students are glaringly indifferent to what professors have to say, so professors have more latitude than businesspeople to benefit from exaggerated and duplicitous claims.
Professors have to be more restrained when publishing than when teaching because other professors will evaluate the truth of their published claims. It is true that academic promotions may be earned and scholarly reputations enhanced by exposing the errors in published work. However, professors are often less concerned with the truthfulness of articles written by other professors than one might think. Professors anxious to get their own articles and books published are often less interested in whether the publications they cite are correct than in whether the publications are accepted as correct by academics with views similar to their own—the people most likely to decide whether their books and articles will be published and cited.
Adapted from D. Lee, “Why Businessmen Are More Honest than Preachers, Politicians, and Professors.” ©2010 The Independent Review.

",?,?
7,passage,The happy American,"
Americans are a “positive” people. This is their reputation as well as their self-image. In the well-worn stereotype, they are upbeat, cheerful, and optimistic.
Who would be churlish enough to challenge these happy features of the American personality? Take the business of positive “affect,” which refers to the mood they display to others through their smiles, their greetings, their professions of confidence and optimism. Scientists have found that the mere act of smiling can generate positive feelings within us, at least if the smile is not forced. In addition, recent studies show that happy feelings flit easily through social networks, so that one person’s good fortune can brighten the day even for only distantly connected others. Furthermore, psychologists agree that positive feelings can actually lengthen our lives and improve our health. People who report having positive feelings are more likely to participate in a rich social life, and social connectedness turns out to be an important defense against depression, which is a known risk factor for many physical illnesses.
It is a sign of progress, then, that economists have begun to show an interest in using happiness rather than just the gross national product as a measure of an economy’s success. Happiness is, of course, a slippery thing to measure or define. Philosophers have debated what it is for centuries, and even if they were to define it simply as a greater frequency of positive feelings than negative ones, when they ask people if they are happy, they are asking them to arrive at some sort of average over many moods and moments.
Surprisingly, when psychologists measure the relative happiness of nations, they routinely find that Americans are not, even in prosperous times and despite their vaunted positivity, very happy at all. A recent meta-analysis of over a hundred studies of self-reported happiness worldwide found Americans ranking only twenty-third. Americans account for two-thirds of the global market for antidepressants, which happen also to be the most commonly prescribed drugs in the United States.
How can Americans be so surpassingly “positive” in self-image and stereotype without being the world’s happiest and best-off people? The answer is that positivity is not so much their condition as it is part of their ideology—the way they explain the world and think they ought to function within it. That ideology is “positive thinking,” by which they usually mean two things. One is the generic content of positive thinking—that is, the positive thought itself—which can be summarized as “Things are pretty good right now, at least if you are willing to see silver linings, make lemonade out of lemons, etc., and things are going to get a whole lot better.”
The second thing they mean by “positive thinking” is this practice of trying to think in a positive way. There is, they are told, a practical reason for undertaking this effort: positive thinking supposedly not only makes us feel optimistic but actually makes happy outcomes more likely. How can the mere process of thinking do this? In the rational explanation that many psychologists would offer today, optimism improves health, personal efficacy, confidence, and resilience, making it easier for us to accomplish our goals. A far less rational theory also runs rampant in American ideology—the idea that our thoughts can, in some mysterious way, directly affect the physical world. Negative thoughts somehow produce negative outcomes, while positive thoughts realize themselves in the form of health, prosperity, and success. For both rational and mystical reasons, then, the effort of positive thinking is said to be well worth our time and attention.
Adapted from B. Ehrenreich, Bright-sided. ©2009 by Metropolitan Books.

",?,social commentary
8,passage,Designing Courthouses,"In a recent study, psychologist Anne Maass investigated the effects of courthouse architecture on the psychological well-being and cognitive processes of potential users. Specifically, she compared two courthouses located in Padova, Italy: the old courthouse, located in a former convent originally built in 1345, and the new courthouse, built in 1991 and designed by Gino Valle, an internationally known architect. Although serving or having served the same purpose, the two buildings have completely different styles—one is an old building with a rather residential look, warm colors, large windows, and a large wooden door, the other a massive, gray, semi-circular building, with narrow windows, and an entrance enclosed between two huge walls.
When study participants were asked to imagine themselves accompanying a friend to the courthouse, they reported greater discomfort and stress when anticipating a trial in the modern building. However, contrary to predictions, this was true only when they were already familiar with the two buildings. It is possible that photographs reduced the actual impact of the architectural design, although this would contradict prior research by architect Gavin Stamp showing that distortions due to photographic presentation have negligible effects on preference. Another possibility for participants’ greater discomfort when imagining going to the new courthouse is that those with prior experience may have been exposed to the building from multiple angles, whereas unfamiliar participants received information only about the building’s facade.
It is important to note that participants did not generally dislike the new building. From the standpoint of general aesthetic distinctions such as beauty versus ugliness, no differences emerged between the two buildings; if anything, the new building was seen by the participants as slightly more attractive. The data suggest that participants responded more to the intimidating nature of the building than to its beauty.
The most important result of Maass’s research is that courthouse architecture was found to affect the estimated likelihood of conviction. Participants were more pessimistic about the trial outcome when they imagined entering the new building than when they imagined entering the old one. (This occurred regardless of whether participants had any prior familiarity with the respective buildings.) It remains unclear exactly which architectural features are responsible for the observed shift in likelihood of conviction estimates. The modern building differs on so many dimensions (size, color, shape, building materials, age, and so on) from the old building that it is impossible to isolate their individual impact. Also, it may be the interaction of features that creates the overall impression of the building as intimidating.
How exactly do architectural features affect social-cognitive processes such as likelihood estimates? One possibility is that design features affect the emotional well-being or mood of the user which, in turn, biases his or her thought processes. For example, the architectural characteristics of the new courthouse seem to have made hypothetical users feel anxious and tense, and a bad mood has been shown to induce negative thoughts and expectations. However, building type affected perceived likelihood of conviction also for those participants who showed no enhanced discomfort in reaction to the new building.
Another and more plausible possibility is that the design features of the new courthouse activated specific thoughts and mental associations related to conviction. For example, some participants spontaneously commented that the new building has greater resemblance to a prison than to a courthouse; others mentioned that the two high walls enclosing the entrance give the impression that those who enter the building are already convicted.
Adapted from A. Maass, “Intimidating Buildings: Can courthouse architecture affect perceived likelihood of conviction?” Environment and Behavior. ©2000 by Sage Publications, Inc.

",social science,psychology
9,passage,Seeing color through Homer’s eyes,"For someone used to contemporary academic writing, reading the chapter on color in William Gladstone’s Studies on Homer and the Homeric Age (1858) comes as rather a shock—the shock of meeting an extraordinary mind. It is therefore all the more startling that Gladstone’s nineteenth century tour de force comes to such a strange conclusion: Homer and his contemporaries perceived the world in something closer to black and white than to full Technicolor.
No one would deny that there is a wide gulf between Homer’s world and ours: in the millennia that separate us, empires have risen and fallen, religions and ideologies have come and gone, and science and technology have transformed our intellectual horizons and almost every aspect of daily life beyond all recognition. Surely one aspect that must have remained exactly the same since Homer’s day, even since time immemorial, would be the rich colors of nature: the blue of sky and sea, the glowing red of dawn, the green of fresh leaves.
Gladstone says things are not the same, for many reasons. One, Homer uses the same word to denote colors which, according to us, are essentially different. For example, he describes as “violet” the sea, sheep, and iron. Two, Homer’s similes are so rich with sensible imagery, we expect to find color a frequent and prominent ingredient, and yet his poppies have never so much as a hint of scarlet. Three, Gladstone notes, Homer uses “black” about 170 times, “white” 100 times, “red” thirteen, “yellow” ten, “violet” six times, and the other colors even less often. Four, Homer’s color vocabulary is astonishingly small. There doesn’t seem to be anything equivalent to our orange or pink in Homer’s color palette; most striking is the lack of any word that could be taken to mean “blue.”
What is more, Gladstone proves that the oddities in Homer’s Iliad and Odyssey could not have stemmed from any problems peculiar to Homer. “Violet-colored hair” was used by Pindar in his poems.
Gladstone is well aware of the utter weirdness of his thesis—nothing less than universal color blindness among the ancient Greeks—so he tries to make it more palatable by evoking an evolutionary explanation for how sensitivity to colors could have increased over the generations. The perception of color, he says, seems natural to us only because humankind as a whole has undergone a progressive “education of the eye” over the last millennia. The eye’s ability to perceive and appreciate differences in color, he suggests, can improve with practice, and these acquired improvements are then passed on to offspring.
But why, one may well ask, should this progressive refinement of color vision not have started much earlier than the Homeric period? Gladstone’s theory is that the appreciation of color as a property independent of a particular material develops only with the capacity to manipulate colors artificially. And that capacity, he notes, barely existed in Homer’s day: the art of dyeing was in its infancy, cultivation of flowers was not practiced, and almost all of the brightly colored objects we take for granted were entirely absent. Other than the ocean, people in Homer’s day may have gone through life without ever setting their eyes on a single blue object. Blue eyes, Gladstone explains, were in short supply; blue dyes, which are very difficult to manufacture, were practically unknown; and natural flowers that are truly blue are rare.
Gladstone’s analysis was brilliant, but completely off course. Indeed, philologists, anthropologists, and even natural scientists would need decades to free themselves from the error of underestimating the power of culture.
Adapted from G. Deutscher, Through the language glass: why the world looks different in other languages. ©2010 Metropolitan Books.

",humanities,literature
10,passage,Physical Education in the United Kingdom,"In the United Kingdom, Physical Education (PE) is compulsory in state schools until students reach the age of 16. That is, sports are compulsory for as long as formal education is mandated by law. Because there are many children who don’t want to participate in PE classes, I believe that students should be allowed a choice. If their parents agree, why should they be forced to jump on a trampoline or do calisthenics? PE class is different from other classes because it involves what one does with one’s body. We acknowledge the right of individuals to control their own bodies—to determine whether and when they have an operation, to determine where they go and what they do. Why is this any different?
It is a red herring to say that PE makes any serious difference to people’s health. There are more effective ways of ensuring a healthy population than pushing children to run laps around a freezing sports field once a week. For example, schools could be addressing the poor diets young people have today and encouraging them to walk or bicycle to school rather than rely on the car.
Furthermore, sports are a waste of school time and resources. One or two PE lessons a week make very little difference to an individual’s health, but they make a huge difference in a school’s budget. Mandatory PE requires a whole extra department in schools, wasting a great deal of money and time that could be better spent on academics. It also requires schools to be surrounded by a large amount of land for playing fields, making it prohibitively expensive to build new schools in urban areas. Given the average current pupil–teacher ratios, the quality of teaching in PE classes is necessarily low, and the classes may even be dangerous to students who are not properly supervised. Our children are burdened enough in schools already, especially at the older end of the system, with multiple examinations. PE simply adds, needlessly, to this hectic schedule.
Many people argue that playing team sports builds character, encourages students to work with others, teaches children how to win and lose with good grace, and builds strong school spirit through competition with other institutions. It is often, they say, the experience of playing on a team together which builds the strongest friendships at school, friendships which endure for years afterwards. Many say the same benefits derive from the common endurance of prison.
Injuries sustained through school sport and the psychological trauma of being bullied for sporting ineptitude can mark people for years after they have left school. On that note, in an increasingly litigious age, a compulsory rather than voluntary sports program is a liability. More and more schools are avoiding team games such as rugby, soccer, hockey, and football due to the realistic fear of lawsuits. Teamwork can be better developed through music, drama, and community projects without the need to encourage an ultra-competitive ethos.
As for the argument that without compulsory PE, many members of society wouldn’t find out that they had a talent for a sport or even that they enjoyed it, students can discover this aptitude outside of school, without also discovering the bullying and humiliation that comes with PE classes more than with other lessons. The aim of compulsory PE isn’t being fulfilled at present in any case, as “sick notes” are produced with alarming regularity by parents complicit in their children’s wish to avoid it. Greater efforts to enforce it will only result in more deceit, children missing school for the entire day, or, in the most extreme cases, children being withdrawn from state education.
Adapted from A. Deane, “Physical Education, Compulsory,” Creative Commons. ©2011 Creative Commons.

",social science,?
11,passage,Censorship: An Unnecessary Evil,"Today’s parents face a tough battle. Neighborhoods are a lot more complicated than they were in the 1960s: every culture, every religion, every idea, every different standard, lives right next door. Information is received at lightning speed via the Internet, and children can be caught up in this whirlwind, subjected to things that they are still too young to understand or are emotionally unfit to handle.
Censorship seems to be an answer to the growing problem of how to care for and watch over our children. But books are meant for exploration, for questioning. Within a book’s pages, children are safe to explore their feelings and reflect on their own situations. Putting the right book into the hands of the right child has great value and changes lives. It can be empowering, motivating, and inspiring.
Here in the United States, an ostensibly free country, one where people are encouraged and given the legal right to speak their minds, we have been balancing personal freedoms and rights. But our media challenge this balance every day. As consumers, we respect artists and allow them the freedom of expression. At the same time, we are aware that children are seeing some unsuitable situations—but we are not always in agreement about what we want our children to watch, hear, or read.
One political solution is rating systems, intended to help parents pick appropriate material for their children based on content, theme, violence, language, nudity, sensuality, drug abuse, and other elements. However, the rating systems have not stopped today’s lyrics from becoming more explicit, our cable television system from containing more swearing and sexual content, and our movies from becoming bloodier and more violent. And despite all the warnings and all the ratings, children are still listening to these songs, watching these television shows, and renting these movies. The rating system may have convinced politicians, parents, and librarians that it could do the job of protecting their young. It may have given people a false sense of security. But in reality, it means nothing when no one is there to monitor children’s actions and discuss appropriate behavior.
Parents have a vested interest in their children. Creating a home in which a child feels safe is their responsibility. Creating a home where a child can safely make mistakes is their responsibility. Home is the first place where a child learns right from wrong, good from bad, healthy from unhealthy. It is the parents’ job to give their child a good defense by helping them establish boundaries.
School helps to reinforce these lessons. Teachers help children by challenging them, instructing them, and helping them move on to the next level of maturity and understanding. A teacher may know, before a parent, when a child is ready for the next level or is mature enough to handle a theme or topic. When there is communication and respect between parent and teacher, the child’s development is the winner.
America is a free society and has plenty of forums where people can express their views: newspapers, radio, billboards, and the Internet. People can discuss their differences and learn from each other. Why shouldn’t we allow our children that same rich experience? Banning a book is about as helpful as using a match in a hurricane. It does not shed light on anything and gets blown around by a lot of wind. Nor does sticking a label on a problem make it go away. Only in discussing, in sharing comments and concerns, is there growth and understanding. Let us show our children that knowledge is the most empowering censor they can use.
Adapted from L. Caravette, “Censorship: An unnecessary evil,” The Looking Glass : New Perspectives on Children's Literature. ©2008 New Perspectives on Children's Literature.

",?,?